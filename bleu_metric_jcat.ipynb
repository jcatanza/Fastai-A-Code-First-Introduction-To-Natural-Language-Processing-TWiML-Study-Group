{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was created by Sylvain Gugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the BLEU metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU metric has been introduced in [this article](https://www.aclweb.org/anthology/P02-1040) to come with some kind of way to evaluate the performance of translation models. It's based on the precision you hit with n-grams in your prediction compared to your target. Let's see this on an example. Imagine you have the target sentence\n",
    "```\n",
    "the cat is walking in the garden\n",
    "```\n",
    "and your model gives you the following output\n",
    "```\n",
    "the cat is running in the fields\n",
    "```\n",
    "We are going to compute the precision, which is the number of correctly predicted n-grams divided by the number of predicted n-grams for n going from 1 to 4.\n",
    "\n",
    "For 1-grams (or tokens, more simply), we have predicted 5 correct words out of 7, so we get a precision of 5/7. Note that the order doesn't matter, for instance predicting\n",
    "```\n",
    "she read the book because she was interested in world history\n",
    "```\n",
    "instead of\n",
    "```\n",
    "she was interested in world history because she read the book\n",
    "```\n",
    "would give a precision of 1 for the 1-grams.\n",
    "\n",
    "For 2-grams, in the first example, we have 3 correct 2-grams (\"the cat\", \"cat is\" and \"in the\") out of 6, so a precision of 3/6. In the second example, the precision for 2-grams is 9/10.\n",
    "\n",
    "For 3-grams, in the first example, we have only 1 correct 3-gram (\"the cat is\") out of 5, so a precision of 1/5. In the second example, the precision for 3-grams is 6/9.\n",
    "\n",
    "For 4-grams, in the first example, we don't have any 4-gram that is correct, so the precision is 0. In the second example, it's 4/8.\n",
    "\n",
    "There is one big drawback in just taking the raw precision: very often a seq2seq model will predict the same easy word. If take the prediction\n",
    "```\n",
    "the the the the the the the the\n",
    "```\n",
    "for our first example, it would score a precision of 1. for 1-grams (because `the` is in the target sentence, so all the words are considered correct). To avoid that, we put a maximum for a given words to the number of times it can be considered correct, which is the number of times that word is in the target sentence. So in our example, only 2 of the 7 `the` are considered correct and this clamped precision gives us 2/7 for 1-grams.\n",
    "\n",
    "One thing to note is that when we deal with a whole corpus, we take the sum of all the corrects over all the sentences then divide by the sum of all the predicted over all the sentences (we don't avarage precisions over each sentence).\n",
    "\n",
    "To compute the BLEU score, the final formula is then\n",
    "```\n",
    "BLEU = length_penalty * ((p1 * p2 * p3 * p4) ** 0.25)\n",
    "```\n",
    "which is the geometric average of `p1`, `p2`, `p3` and `p4` (our n-gram precision scores) multiplied by a penalty given for the length: we penalize longer predictions with the precision scores, but short ones get it easier, especially if they only contain correct words. So we apply the following penalty:\n",
    "```\n",
    "length_penalty = 1 if len(pred) >= len(targ) else (1 - exp(-len(targ)/len(pred)))\n",
    "\n",
    "NOte there should be a negative sign before the exponent.\n",
    "```\n",
    "\n",
    "And if we are taking the BLEU score of a whole corpus, we use the sum of the lengths of predicted sentences and the sum of the lengths of predicted targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an implementation of BLEU in nltk, but the problem is that it's designed to support lists of tokenized texts, and therefore is very slow (5 hours announced on the validation set of the translation notebook for the average of BLEU scores of sentences). We have numericalized text, so it's easier to reimplement this and only deal with integers.\n",
    "\n",
    "Specifically we are going to use the Counter class, which is going to count the number of instances of each n-gram in the predicted sentence and the target one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bleu score for unigrams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose we have two lists of integers that represent word sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ = [1,2,3,4,5,1,2]\n",
    "pred = [1,2,3,7,5,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Counter object for each list, which is essentially a dictionary of items and number of occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_pred,cnt_targ = Counter(pred),Counter(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({1: 3, 2: 1, 3: 1, 7: 1, 5: 1}),\n",
       " Counter({1: 2, 2: 2, 3: 1, 4: 1, 5: 1}))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_pred,cnt_targ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Bleu score is the number of corrects (the number of words in `pred` that are in `targ`) with a cap that is the number of times they appear in `targ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrects = sum([min(c, cnt_targ[g]) for g,c in cnt_pred.items()])\n",
    "corrects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(1, 3), (2, 1), (3, 1), (7, 1), (5, 1)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_pred.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([3, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_pred.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works for unigrams, which are represented as ints. \n",
    "But it won't work for an ngram of more than one word, which is represented as a `list of ints`, since a `Counter` requires the objects inside to be `hashable`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bleu score for ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a custom class that maps any ngram to a (hopefully unique) integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGram contains methods __eq__ and __hash__\n",
    "# is the object self.ngram an embedding of a sequence of words?\n",
    "class NGram():\n",
    "    def __init__(self, ngram, max_n=5000): self.ngram,self.max_n = ngram,max_n\n",
    "    def __eq__(self, other):\n",
    "        if len(self.ngram) != len(other.ngram): return False\n",
    "        return np.all(np.array(self.ngram) == np.array(other.ngram))\n",
    "    # Generate an integer hash for each ngram\n",
    "    def __hash__(self):\n",
    "        # suppose the ngram is \"I see\"\n",
    "        #      then enumerate(self.ngram) is [0,stoi[\"I\"]], [1,stoi[\"see\"]]\n",
    "        #      then output is [ stoi[\"I\"]*5000**0, stoi[\"see\"]*5000**1 ]\n",
    "        return int(sum([o * self.max_n**i for i,o in enumerate(self.ngram)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `max_n` should be set to the vocab size, so we're sure we don't have two different ngrams with the same hash (it needs to be an int).\n",
    "\n",
    "#### !!!!! NOTE: It's not clear that the above algorithm generates unique hash for each ngram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function to gather all the possible ngrams of a sequence of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input object x is a sequential list of words, (or integer representations of words?)\n",
    "def get_grams(x, n, max_n=5000):\n",
    "    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.NGram at 0x14a8cdbd630>,\n",
       " <__main__.NGram at 0x14a8cdbd668>,\n",
       " <__main__.NGram at 0x14a8cdbd588>,\n",
       " <__main__.NGram at 0x14a8cdbd4e0>,\n",
       " <__main__.NGram at 0x14a8cdbd518>,\n",
       " <__main__.NGram at 0x14a8cdbd4a8>,\n",
       " <__main__.NGram at 0x14a8cdbd6a0>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = get_grams([\"I\",\"see\",\"that\",\"you\",\"like\",\"to\",\"run\",\"fast\"],2,max_n=5000)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class '__main__.NGram'>\n"
     ]
    }
   ],
   "source": [
    "print(type(xx))\n",
    "print(type(xx[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'max_n',\n",
       " 'ngram']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(xx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__func__',\n",
       " '__ge__',\n",
       " '__get__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__self__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(xx[0].__hash__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['I', 'see'],\n",
       " ['see', 'that'],\n",
       " ['that', 'you'],\n",
       " ['you', 'like'],\n",
       " ['like', 'to'],\n",
       " ['to', 'run'],\n",
       " ['run', 'fast']]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[xx[i].ngram for i in range(len(xx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[0].ngram.__eq__(xx[1].ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx[0].ngram.__eq__(xx[0].ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input should actually be a list of integer representations of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.NGram at 0x14a8cdd2b38>,\n",
       " <__main__.NGram at 0x14a8cdd2588>,\n",
       " <__main__.NGram at 0x14a8cdd2898>,\n",
       " <__main__.NGram at 0x14a8cdd28d0>,\n",
       " <__main__.NGram at 0x14a8cdd27f0>,\n",
       " <__main__.NGram at 0x14a8cdd26a0>]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy = get_grams([1,5,6,10,2,1,10,6],3,max_n=5000)\n",
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 5, 6], [5, 6, 10], [6, 10, 2], [10, 2, 1], [2, 1, 10], [1, 10, 6]]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[yy[i].ngram for i in range(len(yy))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[150025001, 250030005, 50050006, 25010010, 250005002, 150050001]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[yy[i].__hash__() for i in range(len(yy))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the number of correctly predicted ngrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_ngrams(pred, targ, n, max_n=5000):\n",
    "    # inputs pred, targ are predicted and target word sequences\n",
    "    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)\n",
    "    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n",
    "    return sum([min(c, targ_cnt[g]) for g,c in pred_cnt.items()]),len(pred_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_correct_ngrams([1,5,4,10,1,1,10,6],[1,5,6,10,2,1,10,6],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BLEU metric over two sentences is implemented as follows:\n",
    "There is a penalty for disparity between predicted and target sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_bleu(pred, targ, max_n=5000):\n",
    "    # here we count up to 4-grams\n",
    "    # max_ngram_number\n",
    "    corrects = [get_correct_ngrams(pred,targ,n,max_n=max_n) for n in range(1,max_ngram_number)]\n",
    "    # list with fraction of ngrams found for each ngram_number\n",
    "    n_precs = [c/t for c,t in corrects]\n",
    "    # \n",
    "    # penalty\n",
    "    len_penalty = exp(1 - len(targ)/len(pred)) if len(pred) < len(targ) else 1 # between 0 and 1\n",
    "    # bleu score (for max_ngram_number = 4)\n",
    "    bleu = len_penalty * ((n_precs[0]*n_precs[1]*n_precs[2]*n_precs[3]) ** 0.25)\n",
    "    return bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the BLEU metric over a corpus is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_bleu(preds, targs, max_n=5000):\n",
    "    pred_len,targ_len,n_precs,counts = 0,0,[0]*4,[0]*4\n",
    "    for pred,targ in zip(preds,targs):\n",
    "        pred_len += len(pred)\n",
    "        targ_len += len(targ)\n",
    "        for i in range(4):\n",
    "            c,t = ngram_corrects(pred, targ, i+1, max_n=max_n)\n",
    "            n_precs[i] += c\n",
    "            counts[i] += t\n",
    "    n_precs = [c/t for c,t in zip(n_precs,counts)]\n",
    "    len_penalty = exp(1 - targ_len/pred_len) if pred_len < targ_len else 1\n",
    "    return len_penalty * ((n_precs[0]*n_precs[1]*n_precs[2]*n_precs[3]) ** 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes 11s to run on our validation set (instead of 5 hours), so we can even use it as a metric during training, we have to define it as a `Callback`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusBLEU(Callback):\n",
    "    def __init__(self, vocab_sz):\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.name = 'bleu'\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.pred_len,self.targ_len,self.n_precs,self.counts = 0,0,[0]*4,[0]*4\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        last_output = last_output.argmax(dim=-1)\n",
    "        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n",
    "            self.pred_len += len(pred)\n",
    "            self.targ_len += len(targ)\n",
    "            for i in range(4):\n",
    "                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n",
    "                self.n_precs[i] += c\n",
    "                self.counts[i] += t\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        n_precs = [c/t for c,t in zip(n_precs,counts)]\n",
    "        len_penalty = exp(1 - targ_len/pred_len) if pred_len < targ_len else 1\n",
    "        bleu = len_penalty * ((n_precs[0]*n_precs[1]*n_precs[2]*n_precs[3]) ** 0.25)\n",
    "        return add_metrics(last_metrics, bleu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
