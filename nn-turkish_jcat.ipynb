{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turkish ULMFiT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs=128\n",
    "bs=48\n",
    "# torch.cuda.set_device(2)\n",
    "torch.cuda.set_device(0)\n",
    "data_path = Config.data_path()\n",
    "\n",
    "lang = 'tr'\n",
    "name = f'{lang}wiki'\n",
    "path = data_path/name\n",
    "\n",
    "path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_path = path/'models'\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "lm_fns = [mdl_path/f'{lang}_wt', mdl_path/f'{lang}_wt_vocab']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkish wikipedia model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 999 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# excludes articles with fewer than 1800 words, since they tend to be \"stubs\" with mostly boilerplate.\n",
    "from nlputils import split_wiki,get_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\cross-entropy\\\\.fastai\\\\data\\\\trwiki\\\\text\\\\AA\\\\wiki_00'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\shutil.py\u001b[0m in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\cross-entropy\\\\.fastai\\\\data\\\\trwiki\\\\text\\\\AA\\\\wiki_00' -> 'C:\\\\Users\\\\cross-entropy\\\\.fastai\\\\data\\\\trwiki\\\\trwiki'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-aa2f9a5ff8b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_wiki\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\fastai\\course-nlp\\nlputils.py\u001b[0m in \u001b[0;36mget_wiki\u001b[1;34m(path, lang)\u001b[0m\n\u001b[0;32m     23\u001b[0m         os.system(\"python wikiextractor/WikiExtractor.py --processes 4 --no_templates \" +\n\u001b[0;32m     24\u001b[0m             f\"--min_text_length 1800 --filter_disambig_pages --log_file log -b 100G -q {xml_fn}\")\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;34m'text/AA/wiki_00'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\shutil.py\u001b[0m in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m             \u001b[0mcopy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\cross-entropy\\\\.fastai\\\\data\\\\trwiki\\\\text\\\\AA\\\\wiki_00'"
     ]
    }
   ],
   "source": [
    "get_wiki(path,lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download failed but provided a helpful error message!\n",
    "Download of https://dumps.wikimedia.org/trwiki/latest/trwiki-latest-pages-articles.xml.bz2 has failed after 5 retries\n",
    "\n",
    "We'll install the files manually, following instructions provided in the error message (see below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Windows 10 users, follow these steps if you have not already done so:\n",
    "\n",
    "1) Follow the instructions here to install `Chocolatey`:\n",
    "https://chocolatey.org/install\n",
    "\n",
    "2) In an `administrator PowerShell`, install `wget` version 1.20 (or later?) via this command:\n",
    "\n",
    "`choco install wget --version 1.20`\n",
    "\n",
    "3) Download and install 7-zip for Windows 10: https://www.7-zip.org/download.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix the download manually:\n",
    " \n",
    "\\$ mkdir -p C:\\Users\\cross-entropy\\.fastai\\data\n",
    "\n",
    "\\$ cd C:\\Users\\cross-entropy\\.fastai\\data\n",
    "\n",
    "\\$ wget https://dumps.wikimedia.org/trwiki/latest/trwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "Use 7-zip to extract trwiki-latest-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Directly download trwiki-latest-pages-articles.xml.bz2 from https://archive.org/details/wikipediadumps\n",
    "\n",
    "Then extract it with the following code, snipped from nlputils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trwiki-latest-pages-articles.xml\n"
     ]
    }
   ],
   "source": [
    "xml_fn = f\"{lang}wiki-latest-pages-articles.xml\"\n",
    "zip_fn = f\"{xml_fn}.bz2\"\n",
    "\n",
    "if not (path/xml_fn).exists():\n",
    "    print(\"downloading...\")\n",
    "    download_url(f'https://dumps.wikimedia.org/{name}/latest/{zip_fn}', path/zip_fn)\n",
    "    print(\"unzipping...\")\n",
    "    bunzip(path/zip_fn)\n",
    "print(xml_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.4/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.4/ http://www.mediawiki.org/xml/export-0.4.xsd\" version=\"0.4\" xml:lang=\"tr\">\n",
      "  <siteinfo>\n",
      "    <sitename>Wikipedia</sitename>\n",
      "    <base>http://tr.wikipedia.org/wiki/Ana_Sayfa</base>\n",
      "    <generator>MediaWiki 1.16alpha-wmf</generator>\n",
      "    <case>first-letter</case>\n",
      "    <namespaces>\n",
      "      <namespace key=\"-2\">Medya</namespace>\n",
      "      <namespace key=\"-1\">Ã–zel</namespace>\n",
      "      <namespace key=\"0\" />\n",
      "      <namespace key=\"1\">TartÄ±ÅŸma</namespace>\n",
      "      <namespace key=\"2\">KullanÄ±cÄ±</namespace>\n",
      "      <namespace key=\"3\">KullanÄ±cÄ± mesaj</namespace>\n",
      "      <namespace key=\"4\">Vikipedi</namespace>\n",
      "      <namespace key=\"5\">Vikipedi tartÄ±ÅŸma</namespace>\n",
      "      <namespace key=\"6\">Dosya</namespace>\n",
      "      <namespace key=\"7\">Dosya tartÄ±ÅŸma</namespace>\n",
      "      <namespace key=\"8\">MedyaViki</namespace>\n",
      "      <namespace key=\"9\">MedyaViki tartÄ±ÅŸma</namespace>\n",
      "      <namespace key=\"10\">Åžablon</namespace>\n",
      "      <namespace key=\"11\">Åžablon tartÄ±ÅŸma</namespace>\n",
      "      <namespace key=\"12\">YardÄ±m</namespace>\n",
      "      <namespace key=\"13\">YardÄ±m tartÄ±ÅŸma</namespace>\n",
      "      <namespace key=\"14\">Kategori</namespace>\n",
      "      <namespace key=\"15\">Kategori tartÄ±ÅŸma</namespace>\n",
      "      <namespace key=\"100\">Portal</namespace>\n",
      "      <namespace key=\"101\">Portal tartÄ±ÅŸma</namespace>\n",
      "    </namespaces>\n",
      "  </siteinfo>\n",
      "  <page>\n",
      "    <title>Ana Sayfa</title>\n",
      "    <id>4</id>\n",
      "    <revision>\n",
      "      <id>6975448</id>\n",
      "      <timestamp>2010-01-04T02:47:09Z</timestamp>\n",
      "      <contributor>\n",
      "        <username>Ä°nfoCan</username>\n",
      "        <id>14644</id>\n",
      "      </contributor>\n",
      "      <comment>2010 yÄ±lÄ±nÄ±z kutlu olsun!</comment>\n",
      "      <text xml:space=\"preserve\">{{Vikipedi:Anasayfa yeni baÅŸlÄ±k}}\n",
      "{|style=&quot;border-spacing:2px;margin:0px -2px&quot;\n",
      "|class=&quot;MainPageBG&quot; style=&quot;width:50%;border:1.5px solid #cedff2;background-color:#F5FFFA;vertical-align:top; -moz-border-radius:10px&quot;|&lt;div style=&quot;margin: 0px 0px 0px 0px; text-align: left;&quot;&gt;\n",
      "{| width=&quot;100%&quot; cellpadding=&quot;2&quot; cellspacing=&quot;5&quot; style=&quot;vertical-align:top; background-color:#FFFAFA; -moz-border-radius:10px&quot;\n",
      "! style=&quot;background-color:#FFF5EE; font-family:Verdana, Arial, Helvetica, sans-serif; font-size: 100%; border: 1px solid #FAEBD7; text-align: left; padding-left: 7px; -moz-border-radius:10px&quot; |&lt;div style=&quot;float:right;&quot;&gt;{{TÄ±kla2 || image=Wikibar2.png | link=Vikipedi:HaftanÄ±n seÃ§kin maddesi | width=100px | height=24px }}&lt;/div&gt; [[Vikipedi:HaftanÄ±n seÃ§kin maddesi|HaftanÄ±n seÃ§kin maddesi]]\n",
      "|-\n",
      "| style=&quot;font-family:Verdana, Arial, Helvetica, sans-serif; font-size: 90%; -moz-border-radius:10px&quot; | {{Vikipedi:HaftanÄ±n seÃ§kin maddesi/{{CURRENTWEEK}}. Hafta {{CURRENTYEAR}}}} &lt;span style=&quot;float:right; font-family:Verdana, Arial, Helvetica, sans-serif; font-size: 90%; padding:0 .5em 0 2em;&quot;&gt;\n",
      "|}\n",
      "{{Ana sayfa dinamik iÃ§erik}}&lt;span style=&quot;float:right; font-family:Verdana, Arial, Helvetica, sans-serif; font-size: 90%; padding:0 .5em 0 2em;&quot;&gt;\n",
      "|}\n",
      "|class=&quot;MainPageBG&quot; style=&quot;width:50%;border:1.5px solid #cedff2;background-color:#FAF5FF;vertical-align:top;color:#000; -moz-border-radius:10px&quot;|&lt;div style=&quot;margin: 1px 0px 0px 0px; text-align: left; -moz-border-radius:10px&quot;&gt;\n",
      "{| width=&quot;100%&quot; cellpadding=&quot;2&quot; cellspacing=&quot;3&quot; style=&quot;vertical-align: top; background-color:#FFFFF0; -moz-border-radius:10px;&quot;\n",
      "|-\n",
      "! style=&quot;background-color:#F5F5DC; font-family:Verdana, Arial, Helvetica, sans-serif; font-size: 100%; border: 1px solid#EEE8AA; text-align: left; padding-left: 7px; -moz-border-radius:10px&quot; |&lt;div style=&quot;float:right;&quot;&gt;{{TÄ±kla2 || image=Wikibar2.png | link=Vikipedi:GÃ¼nÃ¼n seÃ§kin resmi | width=100px | height=24px }}&lt;/div&gt; [[Vikipedi:GÃ¼nÃ¼n seÃ§kin resmi|GÃ¼nÃ¼n seÃ§kin resmi]]\n",
      "|-\n",
      "|{{GSR koruma/{{#time:Y-m-d}}}}\n",
      "|}\n",
      "{| width=&quot;100%&quot; cellpadding=&quot;2&quot; cellspacing=&quot;5&quot; style=&quot;vertical-align:top; background-color:#FAF5FF;  -moz-border-radius:10px&quot;\n",
      "! style=&quot;background-color:#F2E6FF; font-family:Verdana, Arial, Helvetica, sans-serif; font-size: 100%; border: 1px solid #D8BFD8; text-align: left; padding-left: 7px; -moz-border-radius:10px&quot; | &lt;div style=&quot;float:right;&quot;&gt;{{TÄ±kla2 || image=Wikibar2.png | link=Vikipedi:Tarihte bugÃ¼n | width=100px | height=24px }}&lt;/div&gt;[[Vikipedi:Tarihte bugÃ¼n|Tarihte bugÃ¼n]]\n",
      "|-\n",
      "| style=&quot;font-family:Verdana, Arial, Helvetica, sans-serif; font-size: 90%; -moz-border-radius:10px&quot; | {{Vikipedi:Tarihte bugÃ¼n/{{CURRENTDAY}} {{CURRENTMONTHNAME}}}} &lt;span style=&quot;float:right; font-family:Verdana, Arial, Helvetica, sans-serif; font-size: 90%; padding:0 .5em 0 2em;&quot;&gt;\n",
      "|}\n",
      "|}\n",
      "&lt;br /&gt;\n",
      "{{Ana sayfa metakutu}}\n",
      "{{Ana sayfa lisans2}}\n",
      "\n",
      "__notoc__\n",
      "__noeditsection__\n",
      "\n",
      "\n",
      "&lt;!-- interwiki --&gt;\n",
      "[[ar:]]\n",
      "[[az:]]\n",
      "[[ba:]]\n",
      "[[bg:]]\n",
      "[[bs:]]\n",
      "[[ca:]]\n",
      "[[crh:]]\n",
      "[[cs:]]\n",
      "[[cv:]]\n",
      "[[da:]]\n",
      "[[de:]]\n",
      "[[el:]]\n",
      "[[en:]]\n",
      "[[eo:]]\n",
      "[[es:]]\n",
      "[[et:]]\n",
      "[[eu:]]\n",
      "[[fa:]]\n",
      "[[fi:]]\n",
      "[[fr:]]\n",
      "[[he:]]\n",
      "[[hi:]]\n",
      "[[hr:]]\n",
      "[[hu:]]\n",
      "[[id:]]\n",
      "[[it:]]\n",
      "[[ja:]]\n",
      "[[ka:]]\n"
     ]
    }
   ],
   "source": [
    "!head -n100 {path}/trwiki-latest-pages-articles.xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/xml_fn).exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use WikiExtractor to process the .xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cross-entropy\\.fastai\\data\\trwiki\\trwiki-latest-pages-articles.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xml_fn_full = path/xml_fn\n",
    "print(xml_fn_full)\n",
    "xml_fn_full.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/cross-entropy/.fastai/data/trwiki')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cross-entropy\\fastai\\course-nlp\n"
     ]
    }
   ],
   "source": [
    "path_nlp = Path(r'C:\\Users\\cross-entropy\\fastai\\course-nlp')\n",
    "print(path_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = path_nlp.joinpath(r'C:\\Users\\cross-entropy\\fastai\\course-nlp\\text\\AA\\wiki_00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/cross-entropy/fastai/course-nlp/text/AA/wiki_00')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pp.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/cross-entropy/fastai/course-nlp/text/AA/wiki_00')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_nlp/'text/AA/wiki_00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cross-entropy\\\\fastai\\\\course-nlp\\\\text\\\\AA\\\\wiki_00'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(path_nlp/'text/AA/wiki_00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\cross-entropy\\\\.fastai\\\\data\\\\trwiki\\\\trwiki'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(path/name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trwiki'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/cross-entropy/.fastai/data/trwiki/trwiki')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path/name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/cross-entropy/.fastai/data/trwiki/text')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path/'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"python C:/Users/cross-entropy/wikiextractor/WikiExtractor.py --processes 4 --no_templates \" +\n",
    "    f\"--min_text_length 1800 --filter_disambig_pages --log_file log -b 100G -q {xml_fn_full}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\cross-entropy\\\\fastai\\\\course-nlp\\\\text\\\\AA\\\\wiki_00'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\shutil.py\u001b[0m in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 563\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    564\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\cross-entropy\\\\fastai\\\\course-nlp\\\\text\\\\AA\\\\wiki_00' -> 'C:\\\\Users\\\\cross-entropy\\\\.fastai\\\\data\\\\trwiki\\\\trwiki'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-3cae3e15bfae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_nlp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;34m'text/AA/wiki_00'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\shutil.py\u001b[0m in \u001b[0;36mmove\u001b[1;34m(src, dst, copy_function)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m             \u001b[0mcopy_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopy2\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m     \u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m     \u001b[0mcopystat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[1;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\cross-entropy\\\\fastai\\\\course-nlp\\\\text\\\\AA\\\\wiki_00'"
     ]
    }
   ],
   "source": [
    "shutil.move(str(path_nlp/'text/AA/wiki_00'), str(path/name))\n",
    "shutil.rmtree(path/'text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cross-entropy\\.fastai\\data\\trwiki\\docs already exists; not splitting\n"
     ]
    }
   ],
   "source": [
    "dest = split_wiki(path,lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turkish is an [Agglutinative_language](https://en.wikipedia.org/wiki/Agglutinative_language) so it needs special care!\n",
    "\n",
    "![Turkish morphemes example](images/turkish.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (TextList.from_folder(dest, processor=[OpenFileProcessor(), SPProcessor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextList (0 items)\n",
       "\n",
       "Path: C:\\Users\\cross-entropy\\.fastai\\data\\trwiki\\docs"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2 = path/'trwiki-latest-pages-articles.xml.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/cross-entropy/.fastai/data/trwiki/trwiki-latest-pages-articles.xml.bz2')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OS\n",
      " Volume Serial Number is 5A82-462F\n",
      "\n",
      " Directory of C:\\Users\\cross-entropy\\.fastai\\data\\trwiki\n",
      "\n",
      "02/21/2020  12:31 PM        19,922,944 trwiki-latest-pages-articles.xml.bz2\n",
      "               1 File(s)     19,922,944 bytes\n",
      "               0 Dir(s)  172,161,990,656 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir {path2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tr_databunch'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{lang}_databunch'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `sentencepiece` to tokenize into sub-word units, with a vocab of 30,000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal: C:\\projects\\sentencepiece\\src\\trainer_interface.cc(49) [(trainer_spec.vocab_size()) > (0)] ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-68fbaa18ae4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m data = (TextList.from_folder(dest, processor=[OpenFileProcessor(), SPProcessor()])\n\u001b[1;32m----> 2\u001b[1;33m         \u001b[1;33m.\u001b[0m\u001b[0msplit_by_rand_pct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mlabel_for_lm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         .databunch(bs=bs, num_workers=1))\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\data_block.py\u001b[0m in \u001b[0;36m_inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_item_lists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelLists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_inner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\data_block.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;34m\"Process the inner datasets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[0mxp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_processors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlists\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m         \u001b[1;31m#progress_bar clear the outputs so in some case warnings issued during processing disappear.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\data_block.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, xp, yp, name, max_warn_items)\u001b[0m\n\u001b[0;32m    712\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mfilt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mfilt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    715\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\data_block.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, processor)\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlistify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\text\\data.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, ds)\u001b[0m\n\u001b[0;32m    462\u001b[0m                     for t in progress_bar(ds.items, leave=False)]\n\u001b[0;32m    463\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_model\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_vocab\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m             \u001b[0mcache_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;34m'spm.model'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;34m'spm.vocab'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocab'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\fastai\\text\\data.py\u001b[0m in \u001b[0;36mtrain_sentencepiece\u001b[1;34m(texts, path, pre_rules, post_rules, vocab_sz, max_vocab_sz, model_type, max_sentence_len, lang, char_coverage, tmp_dir, enc)\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;34mf\"--unk_id={len(defaults.text_spec_tok)} --pad_id=-1 --bos_id=-1 --eos_id=-1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;34mf\"--user_defined_symbols={','.join(spec_tokens)}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m         f\"--model_prefix={cache_dir/'spm'} --vocab_size={vocab_sz} --model_type={model_type}\"]))\n\u001b[0m\u001b[0;32m    435\u001b[0m     \u001b[0mraw_text_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Internal: C:\\projects\\sentencepiece\\src\\trainer_interface.cc(49) [(trainer_spec.vocab_size()) > (0)] "
     ]
    }
   ],
   "source": [
    "data = (TextList.from_folder(dest, processor=[OpenFileProcessor(), SPProcessor()])\n",
    "        .split_by_rand_pct(0.1, seed=42)\n",
    "        .label_for_lm()\n",
    "        .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "data.save(f'{lang}_databunch')\n",
    "len(data.vocab.itos),len(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(dest, f'{lang}_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>▁ele ▁geçirerek ▁politik ▁bir ▁özerklik ▁verdiğini ▁belirtmek le ▁birlikte ▁xxmaj ▁tibet ' in ▁yalnızca ▁1913 - 1950 ▁yılları ▁arasında ▁xxmaj ▁çin ' in ▁politik ▁nüfuz undan ▁çıktığını , ▁bölgenin ▁tarihi ▁olarak ▁xxmaj ▁çin ' e ▁ait ▁olduğunu ▁düşünmektedir . ▁xxmaj ▁tibet ' in ▁kendi ▁kültür ▁ve ▁zenginlik lerinin ▁\" kültürel ▁bir ▁soykırım \" a ▁tabi ▁tutulduğu ▁da ▁iddialar ▁arasındadır . ▁xxmaj ▁çin ▁hükümeti ▁ise ▁bu ▁\" kültürel ▁soykırım \" ▁iddia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁dışı ▁kalan ▁olmasa ▁da , ▁o ▁yarışta ▁lastik ▁üreticisi ▁xxmaj ▁michel in ' le ▁yaşanan ▁problemler den ▁dolayı ▁güvenlik ▁gerekçesiyle ▁xxmaj ▁michel in ▁lastik leri ▁kullanan ▁tüm ▁takımlar ▁çekilmiş , ▁yarış a ▁sadece ▁6 ▁pilot ▁katılmıştır . ▁xxmaj ▁bunların ▁dışında ▁xxmaj ▁ ant ô ni o ▁xxmaj ▁pi zz onia ▁xxmaj ▁i ̇ talya ▁ve ▁sonrasında ▁sezonun ▁geri ▁kalanında ▁xxmaj ▁nick ▁xxmaj ▁he id feld ' in ▁yerine ▁yarışmıştır .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>iye ▁oyunlarından ▁çok ▁farklı ▁bulduğunu ▁belirterek ▁taz iye yi ▁de ▁içinde ▁barındıran ▁benzersiz ▁bir ▁yapıt ▁olduğunu ▁yazdı . ▁&lt; ▁/ ▁doc &gt; ▁xxbos ▁xxmaj ▁anahtar sız ▁şifreleme ▁xxmaj ▁anahtar sız ▁şifreleme , ▁anahtar ▁kullanmaya n ▁kriptografik ▁algoritma lar , ▁veya ▁diğer ▁adlarıyla ▁xxmaj ▁veri ▁xxmaj ▁bütünlüğü ▁ve ▁xxmaj ▁özet ▁xxmaj ▁fonksiyonları , ▁veri ▁bütünlüğünü ▁garanti ▁etmek ▁için ▁kullanılan ▁xxup ▁md 5 , ▁xxup ▁sha -1 , ▁xxup ▁ rip em</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>rib ine ▁bağlı ▁olarak ▁akarsu ▁havza larında ▁artan ▁sediman ▁üretimi ▁f lü v yal ▁sistemlerde ▁etkili ▁olmuştur . ▁20 ▁yy ’ ın ▁ikinci ▁yarısından ▁itibaren ▁akarsular ▁üzerinde ▁çok ▁sayıda ▁baraj ▁yapılmıştır . ▁xxmaj ▁baraj ▁yapımı ▁nedeniyle , ▁akarsu nun ▁taşıdığı ▁sediman ▁miktarı ▁çevre nin ▁tahrip ▁edilmesi ▁nedeniyle ▁artmaktadır . ▁xxmaj ▁akarsu ▁yatakları ndan ▁kum ▁alınması , ▁akarsu nun ▁yatağı nın ▁yeniden ▁düzenlemesi ne ▁neden ▁olmaktadır . ▁xxmaj ▁teras lar ;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>n ür lük \", ▁tek ▁başına ▁sensör ▁çözünürlüğü ne ▁bağlı ▁değildir . ▁\" mak sim um ▁sistem ▁çözünürlüğü \", ▁diğer ▁etkenler ▁( sen s ör ▁üzerinde ▁yer ▁alan ▁filtreler , ▁kullanılan ▁objektif in ▁optik ▁kalitesi ▁ve ▁diyafram ▁açıklığı ▁vb . ) ▁dışarıda ▁tutulur sa ▁aşağıdaki ▁formül ▁ile ▁hesaplanabilir ▁( birim ▁= ▁xxup ▁lp ▁/ ▁xxup ▁mm ▁xxmaj ▁i ̇ ng . : ▁ line ▁pa ir s ▁per ▁mili meter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn = language_model_learner(data, AWD_LSTM, drop_mult=0.1, wd=0.1, pretrained=False).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "lr *= bs/48  # Scale learning rate by batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.337346</td>\n",
       "      <td>4.472464</td>\n",
       "      <td>0.297502</td>\n",
       "      <td>10:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.145160</td>\n",
       "      <td>4.359449</td>\n",
       "      <td>0.304195</td>\n",
       "      <td>10:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.199796</td>\n",
       "      <td>4.386935</td>\n",
       "      <td>0.301542</td>\n",
       "      <td>10:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.132432</td>\n",
       "      <td>4.329662</td>\n",
       "      <td>0.305723</td>\n",
       "      <td>10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.048324</td>\n",
       "      <td>4.241048</td>\n",
       "      <td>0.313223</td>\n",
       "      <td>10:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.943493</td>\n",
       "      <td>4.118834</td>\n",
       "      <td>0.323640</td>\n",
       "      <td>10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.857112</td>\n",
       "      <td>3.969563</td>\n",
       "      <td>0.337475</td>\n",
       "      <td>10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.695721</td>\n",
       "      <td>3.799551</td>\n",
       "      <td>0.354562</td>\n",
       "      <td>10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.602813</td>\n",
       "      <td>3.652005</td>\n",
       "      <td>0.371855</td>\n",
       "      <td>10:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.519019</td>\n",
       "      <td>3.596706</td>\n",
       "      <td>0.379076</td>\n",
       "      <td>10:31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(10, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.to_fp32().save(lm_fns[0], with_opt=False)\n",
    "learn.data.vocab.save(lm_fns[1].with_suffix('.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turkish sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.win.tue.nl/~mpechen/projects/smm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/sgugger/.fastai/data/trwiki/movies/tr_polarity.pos'),\n",
       " PosixPath('/home/sgugger/.fastai/data/trwiki/movies/tr_clas_databunch'),\n",
       " PosixPath('/home/sgugger/.fastai/data/trwiki/movies/models'),\n",
       " PosixPath('/home/sgugger/.fastai/data/trwiki/movies/tr_polarity.neg'),\n",
       " PosixPath('/home/sgugger/.fastai/data/trwiki/movies/tr_data_lm')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_clas = path/'movies'\n",
    "path_clas.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gerçekten harika bir yapim birçok kez izledim ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>her izledigimde hayranlik duydugum gerçek klas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gerçekten tarihi savas filmleri arasinda tarti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aldigi ödülleri sonuna dek hak eden muhtesem b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>özgürlük denilince aklima gelen ilk film.bir b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  pos\n",
       "0  gerçekten harika bir yapim birçok kez izledim ...    1\n",
       "1  her izledigimde hayranlik duydugum gerçek klas...    1\n",
       "2  gerçekten tarihi savas filmleri arasinda tarti...    1\n",
       "3  aldigi ödülleri sonuna dek hak eden muhtesem b...    1\n",
       "4  özgürlük denilince aklima gelen ilk film.bir b...    1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# could not read files encoded in UTF-8; Jeremy found that `iso-8859-9` was often used for Turkish language encoding\n",
    "pos = (path_clas/'tr_polarity.pos').open(encoding='iso-8859-9').readlines()\n",
    "pos_df = pd.DataFrame({'text':pos})\n",
    "pos_df['pos'] = 1\n",
    "pos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>giseye oynayan bir film.mel gibson'in oyunculu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bircok yonden sahip olduklari zayifliklari pop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995 ten bu yana bu tür filmler artti , o zama...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mel gibson tam bir ingiliz düsmani her filmind...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>milliyetçi bir film tavsiye etmiyorum.... \\n</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  pos\n",
       "0  giseye oynayan bir film.mel gibson'in oyunculu...    0\n",
       "1  bircok yonden sahip olduklari zayifliklari pop...    0\n",
       "2  1995 ten bu yana bu tür filmler artti , o zama...    0\n",
       "3  mel gibson tam bir ingiliz düsmani her filmind...    0\n",
       "4       milliyetçi bir film tavsiye etmiyorum.... \\n    0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg = (path_clas/'tr_polarity.neg').open(encoding='iso-8859-9').readlines()\n",
    "neg_df = pd.DataFrame({'text':neg})\n",
    "neg_df['pos'] = 0\n",
    "neg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine train and test data since language model doesn't use the labels!\n",
    "df = pd.concat([pos_df,neg_df], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use same `sentencepiece` processor and vocab\n",
    "data_lm = (TextList.from_df(df, path_clas, cols='text', processor=SPProcessor.load(dest))\n",
    "    .split_by_rand_pct(0.1, seed=42)\n",
    "    .label_for_lm()           \n",
    "    .databunch(bs=bs, num_workers=1))\n",
    "\n",
    "data_lm.save(f'{lang}_clas_databunch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path_clas, f'{lang}_clas_databunch', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>▁bile ▁sinema ▁olgusu nun ▁en ▁üst ▁noktalar in dan . . ▁xxbos ▁gerçekten ▁tarihi ▁sava s ▁filmleri ▁ara si nda ▁tar tis ma siz ▁en ▁iyi si ▁ , ▁12 ▁ yi l ▁boyunca ▁ac aba ▁ikincisi ▁çek ir imi ▁diye ▁bekledi gi m ▁bir ▁film ▁ , bel ki ▁william ▁wallace ▁baba sinin ▁ölümünden ▁sonra ▁amca si ▁yani na ▁al m isti ▁onu ▁ ye tis tir m isti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ka dra j lar i yla ▁konusu yla ▁is le ni siyle ▁insana ▁ iste ▁film ▁böyle ▁çekilir ▁de dir ten ▁kusur suz ▁bir ▁film . ▁xxbos ▁böyle ▁güzel ▁bir ▁yap it ▁olamaz ▁filmde ▁her ▁sey ▁var ▁insani ▁dünya dan ▁ali p ▁gö tür üyor ▁bask a ▁diyar lara ▁film ▁bitti kten ▁sonra ▁epey ▁süre ▁geçmesi ▁gerekiyor ▁tekrar ▁dünya ▁ya ▁dönmek ▁için ▁dikkat ! . ▁xxbos ▁inan ir mi siniz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>▁o ▁kadar ▁etki len mis tim ▁ki . . . özellikle ▁özgürlük ▁ ug ru na ▁sava san ▁william ▁wallace  in ▁is ken ce ▁edilerek ▁idam ▁edilmesi . . . ve ▁sonunda ▁özgürlük ▁diye ▁hay kir isi . . . ha lan ▁unut ami yorum ▁xxrep ▁4 ▁ . ▁xxbos ▁ilk ▁bu ▁filmi ▁sinema da ▁izledi m ▁ve ▁insan in ▁inan di ktan ▁sonra ▁ ne leri ▁yap a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>▁için ▁ister ▁istemez ▁oraya ▁gö tür üyor ▁filmin ▁uzun lu gun a ▁al dan ip ta ▁filmi ▁izlemek ten ▁vazgeçme yin ▁xxrep ▁4 ▁ . ▁xxbos ▁harika ▁bir ▁film di ▁xxrep ▁5 ▁ . ▁xxbos ▁mükemmel ▁ötesi . . ▁ . ▁xxbos ▁hiç ▁ a bart mi yorum ▁hayat im da ▁izledi gi m ▁en ▁iyi ▁film lerden ▁biri ▁diye bilir im . tam ▁bir ▁bas yap it ▁nite ligi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>les iyor . ▁herkes ▁izleme li . . . ▁xxbos ▁tarantino nun ▁en ▁iyi ▁filmlerinden ▁biri . ▁diyalog lar ▁çok ▁iyi . ▁kesinlikle ▁izlenmesi ▁gereken ▁bir ▁film . . ▁xxbos ▁tarantino nun ▁bu ▁filmi ▁kendini ▁belli ▁etti r iyor . hat ta ▁ben ce ▁tarantino nun ▁en ▁iyi ▁filmidir . kendi ne ▁has ▁anlat imi ▁ile ▁bu ▁film ▁hak ka ten ▁sinema ▁sever lerin ▁izleme si ▁gereken ▁bir ▁film .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm = language_model_learner(data_lm, AWD_LSTM, pretrained_fnames=lm_fns, drop_mult=1.0, wd=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lr *= bs/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.762205</td>\n",
       "      <td>4.132825</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.fit_one_cycle(1, lr*10, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.136730</td>\n",
       "      <td>3.976010</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.991336</td>\n",
       "      <td>3.882426</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.768218</td>\n",
       "      <td>3.812365</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.526307</td>\n",
       "      <td>3.795765</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.310204</td>\n",
       "      <td>3.815228</td>\n",
       "      <td>00:08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_lm.unfreeze()\n",
    "learn_lm.fit_one_cycle(5, slice(lr/10,lr*10), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_lm.save(f'{lang}fine_tuned')\n",
    "learn_lm.save_encoder(f'{lang}fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_df(df, path_clas, cols='text', processor=SPProcessor.load(dest))\n",
    "    .split_by_rand_pct(0.1, seed=42)\n",
    "    .label_from_df(cols='pos')\n",
    "    .databunch(bs=bs, num_workers=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, pretrained=False, wd=0.1).to_fp16()\n",
    "learn_c.load_encoder(f'{lang}fine_tuned_enc')\n",
    "learn_c.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=2e-2\n",
    "lr *= bs/48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.456195</td>\n",
       "      <td>0.441739</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.416383</td>\n",
       "      <td>0.359390</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.fit_one_cycle(2, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.413642</td>\n",
       "      <td>0.381773</td>\n",
       "      <td>0.828330</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.407912</td>\n",
       "      <td>0.358319</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.fit_one_cycle(2, lr, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.358337</td>\n",
       "      <td>0.310041</td>\n",
       "      <td>0.869606</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.279921</td>\n",
       "      <td>0.276278</td>\n",
       "      <td>0.892120</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.freeze_to(-2)\n",
    "learn_c.fit_one_cycle(2, slice(lr/(2.6**4),lr), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.258410</td>\n",
       "      <td>0.261940</td>\n",
       "      <td>0.899625</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.183993</td>\n",
       "      <td>0.267610</td>\n",
       "      <td>0.905253</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.freeze_to(-3)\n",
    "learn_c.fit_one_cycle(2, slice(lr/2/(2.6**4),lr/2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='4', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      50.00% [2/4 00:07<00:07]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.115543</td>\n",
       "      <td>0.290134</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.107257</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.911820</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='17' class='' max='74', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      22.97% [17/74 00:01<00:03 0.0947]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_c.unfreeze()\n",
    "learn_c.fit_one_cycle(4, slice(lr/10/(2.6**4),lr/10), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy in Gezici (2018), *Sentiment Analysis in Turkish* is: `75.16%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_c.save(f'{lang}clas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
